
@article{VilhuberAEAPap.Proc.2019,
  timestamp = {2019-12-22T22:00:40Z},
  title = {Report by the {{AEA Data Editor}}},
  volume = {109},
  issn = {2574-0768, 2574-0776},
  language = {en},
  journal = {AEA Papers and Proceedings},
  doi = {10.1257/pandp.109.718},
  author = {Vilhuber, Lars},
  month = may,
  year = {2019},
  pages = {718-729},
  file = {Full Text:/home/vilhuber/Zotero/storage/587KRG5B/2019 - Report by the AEA Data Editor.pdf:application/pdf}
}

@book{NationalAcademiesofSciencesEngineeringandMedicine2019,
  timestamp = {2019-09-21T21:58:51Z},
  address = {{Washington, D.C.}},
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  isbn = {978-0-309-48616-3},
  publisher = {{National Academies Press}},
  doi = {10.17226/25303},
  author = {{National Academies of Sciences, Engineering, and Medicine}},
  year = {2019}
}

@article{ChristianInt.J.Digit.Curation2018,
  timestamp = {2019-12-22T22:00:37Z},
  title = {Operationalizing the {{Replication Standard}}: {{A Case Study}} of the {{Data Curation}} and {{Verification Workflow}} for {{Scholarly Journals}}},
  volume = {13},
  shorttitle = {Operationalizing the {{Replication Standard}}},
  abstract = {In response to widespread concerns about the integrity of research published in scholarly journals, several initiatives have emerged that are promoting research transparency through access to data underlying published scientific findings. Journal editors, in particular, have made a commitment to research transparency by issuing data policies that require authors to submit their data, code, and documentation to data repositories to allow for public access to the data. In the case of the American Journal of Political Science (AJPS) Data Replication Policy, the data also must undergo an independent verification process in which materials are reviewed for quality as a condition of final manuscript publication and acceptance.Aware of the specialized expertise of the data archives, AJPS called upon the Odum Institute Data Archive to provide a data review service that performs data curation and verification of replication datasets. This article presents a case study of the collaboration between AJPS and the Odum Institute Data Archive to develop a workflow that bridges manuscript publication and data review processes. The case study describes the challenges and the successes of the workflow integration, and offers lessons learned that may be applied by other data archives that are considering expanding their services to include data curation and verification services to support reproducible research.},
  number = {1},
  journal = {International Journal of Digital Curation},
  doi = {10.2218/ijdc.v13i1.555},
  author = {Christian, Thu-Mai and {Lafferty-Hess}, Sophia and Jacoby, William and Carsey, Thomas},
  year = {2018}
}

@article{WilsonArXiv160900037Cs2016,
  timestamp = {2018-12-08T15:59:12Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.00037},
  primaryClass = {cs},
  title = {Good {{Enough Practices}} in {{Scientific Computing}}},
  abstract = {We present a set of computing tools and techniques that every researcher can and should adopt. These recommendations synthesize inspiration from our own work, from the experiences of the thousands of people who have taken part in Software Carpentry and Data Carpentry workshops over the past six years, and from a variety of other guides. Unlike some other guides, our recommendations are aimed specifically at people who are new to research computing.},
  urldate = {2018-12-08},
  journal = {arXiv:1609.00037 [cs]},
  url = {http://arxiv.org/abs/1609.00037},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  month = aug,
  year = {2016},
  keywords = {Computer Science - Software Engineering},
  file = {arXiv\:1609.00037 PDF:/home/vilhuber/Zotero/storage/VECBP5J3/Wilson et al. - 2016 - Good Enough Practices in Scientific Computing.pdf:application/pdf;arXiv.org Snapshot:/home/vilhuber/Zotero/storage/S65UCPER/1609.html:text/html}
}

@article{KleinSoc.Psychol.2014,
  timestamp = {2019-12-22T22:00:39Z},
  title = {Investigating {{Variation}} in {{Replicability}}: {{A}} ``{{Many Labs}}'' {{Replication Project}}},
  volume = {45},
  issn = {1864-9335, 2151-2590},
  shorttitle = {Investigating {{Variation}} in {{Replicability}}},
  language = {en},
  number = {3},
  journal = {Social Psychology},
  doi = {10.1027/1864-9335/a000178},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and {van `t Veer}, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  month = may,
  year = {2014},
  pages = {142-152},
  file = {Full Text:/home/vilhuber/Zotero/storage/MNKWJ7F6/Klein et al. - 2014 - Investigating Variation in Replicability A â€œMany .pdf:application/pdf}
}

@techreport{Klein2013,
  timestamp = {2019-12-22T22:00:39Z},
  type = {Preprint},
  title = {Investigating {{Variation}} in {{Replicability}}: {{A}} ``{{Many Labs}}'' {{Replication Project}}},
  shorttitle = {Investigating {{Variation}} in {{Replicability}}},
  abstract = {We conducted replications of 13 effects in psychological science with 36 samples and more than 6000 participants.  We examined heterogeneity in replicability across sample and setting.},
  language = {en},
  urldate = {2018-12-06},
  institution = {{Open Science Framework}},
  url = {https://osf.io/wx7ck/},
  author = {Klein, Richard A. and Ratliff, Kate and Vianello, Michelangelo and Reginald B. Adams, Jr and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael Jason and Bocian, Konrad and Brandt, Mark and Brooks, Beach and Brumbaugh, Claudia},
  month = jun,
  year = {2013},
  file = {Snapshot:/home/vilhuber/Zotero/storage/U23PD87D/wx7ck.html:text/html}
}

@misc{Chetty2012,
  timestamp = {2018-12-03T02:55:13Z},
  title = {Time {{Trends}} in the {{Use}} of {{Administrative Data}} for {{Empirical Research}}},
  urldate = {2018-07-19},
  url = {http://www.rajchetty.com/chettyfiles/admin_data_trends.pdf},
  author = {Chetty, Raj},
  year = {2012},
  file = {admin_data_trends.pdf:/home/vilhuber/Zotero/storage/E22U69XY/admin_data_trends.pdf:application/pdf}
}

@article{JengAm.Econ.Rev.2016,
  timestamp = {2019-12-22T22:00:39Z},
  title = {Making {{Private Data Accessible}} in an {{Opaque Industry}}: {{The Experience}} of the {{Private Capital Research Institute}}},
  volume = {106},
  issn = {0002-8282},
  shorttitle = {Making {{Private Data Accessible}} in an {{Opaque Industry}}},
  abstract = {Private markets are becoming an increasingly important way of financing rapidly growing and mature firms, and private investors are reputed to have far-reaching economic impacts. These important markets, however, are uniquely difficult to study. This paper explores these challenges, as well as the ways they can be overcome, using the experiences of the Private Capital Research Institute as a case.},
  language = {en},
  number = {5},
  journal = {American Economic Review},
  doi = {10.1257/aer.p20161059},
  author = {Jeng, Leslie and Lerner, Josh},
  month = may,
  year = {2016},
  pages = {157-160},
  file = {Snapshot:/home/vilhuber/Zotero/storage/AQMR3IZE/articles.html:text/html;Snapshot:/home/vilhuber/Zotero/storage/ZV3SUIBM/articles.html:text/html}
}

@article{Lagoze2017-qv,
  timestamp = {2018-11-26T22:36:50Z},
  title = {Making Confidential Data Part of Reproducible Research},
  abstract = {The rise of data-centric research practices has uncovered shortcomings in the traditional scholarly communication system. The foundation of that system, the peer-reviewed publication,``[the] selective distribution of ink on paper, or\ldots{} electronic facsimiles of the same''(Bourne, et al., 2011), does not adequately support what has become an essential element of scholarship; the reproducibility of research results. That is, duplicating a ...},
  journal = {Chance},
  url = {http://chance.amstat.org/2017/09/reproducible-research/},
  author = {Lagoze, C and Vilhuber, L},
  year = {2017}
}

@misc{Sporny2014,
  timestamp = {2019-12-22T22:00:40Z},
  title = {{{JSON}}-{{LD}} 1.0},
  urldate = {2018-11-06},
  url = {http://www.w3.org/TR/2014/REC-json-ld-20140116/},
  author = {Sporny, Manu and Longley, Dave and Kellogg, Gregg and Lanthaler, Markus and Lindstr{\"o}m, Niklas},
  year = {2014},
  file = {JSON-LD 1.0:/home/vilhuber/Zotero/storage/52MI5YD3/json-ld.html:text/html}
}

@article{HrynaszkiewiczInt.J.Digit.Curation2017,
  timestamp = {2019-12-22T22:00:39Z},
  title = {Standardising and {{Harmonising Research Data Policy}} in {{Scholary Publishing}}},
  volume = {12},
  issn = {1746-8256},
  number = {1},
  journal = {International Journal of Digital Curation},
  doi = {10.2218/ijdc.v12i1.531},
  author = {Hrynaszkiewicz, Iain and Birukou, Aliaksandr and Astell, Mathias and Swaminathan, Sowmya and Kenall, Amye and Khodiyar, Varsha},
  month = sep,
  year = {2017},
  pages = {65-71},
  file = {Full Text:/home/vilhuber/Zotero/storage/5DNAKG4S/Hrynaszkiewicz et al. - 2017 - Standardising and Harmonising Research Data Policy.pdf:application/pdf}
}

@techreport{NatureScientificData2018,
  timestamp = {2019-12-22T22:00:39Z},
  title = {Nature {{Scientific Data}} Recommended Repositories},
  abstract = {Spreadsheet listing data repositories that are recommended by Scientific Data (Springer Nature) as being suitable for hosting data associated with peer-reviewed articles. Please see the repository list on Scientific Data's website for the most up to date list.},
  institution = {{figshare}},
  doi = {10.6084/m9.figshare.1434640.v12},
  author = {{Nature Scientific Data}},
  month = sep,
  year = {2018},
  keywords = {data sharing,data repositories},
  file = {Figshare Snapshot:/home/vilhuber/Zotero/storage/QFU293JX/1434640.pdf:application/pdf}
}

@misc{Nature-ScientificData,
  timestamp = {2019-12-22T22:00:39Z},
  title = {Recommended {{Data Repositories}} | {{Scientific Data}}},
  copyright = {\textcopyright{}2018 Macmillan Publishers Limited. All Rights Reserved.},
  abstract = {Recommended Data Repositories},
  language = {en},
  urldate = {2018-11-03},
  url = {https://www.nature.com/sdata/policies/repositories},
  author = {{Nature - Scientific Data}},
  file = {Snapshot:/home/vilhuber/Zotero/storage/2LCHVFFW/repositories.html:text/html}
}

@article{Burton2017,
  timestamp = {2019-12-22T22:00:37Z},
  title = {Scholix {{Metadata Schema For Exchange Of Scholarly Communication Links}}},
  abstract = {The goal of the Scholix initiative is to establish a high level interoperability framework for exchanging information about the links between scholarly literature and data. It aims to enable an open information ecosystem to understand systematically what data underpins literature and what literature references data. The DLI Service is the first exemplar aggregation and query service fed by the Scholix open information ecosystem. The Scholix framework together with the DLI aggregation are designed to enable other 3rd party services (domain-specific aggregations, integrations with other global services, discovery tools, impact assessments etc).{$<$}br{$>$}
Scholix is an evolving lightweight set of guidelines to increase interoperability. It consists of: (i) a consensus among a growing group of publishers, datacentres, and global/ domain service providers to work collaboratively and systematically to improve exchange of data-literature link information, (ii) an Information model: conceptual definition of what is a Scholix scholarly link, (iii) Link metadata schema: metadata representation of a Scholix link. Options for exchange protocols (forthcoming) Scholix is the ``wholesaler to wholesaler'' exchange framework, to be implemented by existing hubs or global aggregators of data-literature link information such as DataCite, CrossRef, OpenAIRE, or EMBL-EBI. These hubs in turn work with their natural communities of data centres or literature publishers to collect the information through existing community-specific workflows and standards. Scholix thus enables interoperability between a smaller number of large hubs and leverages the existing exchange arrangements between those hubs and their natural communities (eg between CrossRef and journal publishers). Scholix is a technical solution to wholesale information aggregation; it will need to be complemented by other policy, practice and cultural change advocacy initiatives. This approach could be extended over time to other types of research objects in and beyond research (e.g. software, tweets, etc).},
  language = {en},
  doi = {10.5281/zenodo.1120265},
  author = {Burton, Adrian and Fenner, Martin and Haak, Wouter and Manghi, Paolo},
  month = nov,
  year = {2017}
}

@misc{Rucknagel2015,
  timestamp = {2019-12-22T22:00:40Z},
  title = {Metadata {{Schema}} for the {{Description}} of {{Research Data Repositories}}},
  language = {eng},
  publisher = {{GFZ German Research Center for Geosciences}},
  doi = {10.2312/re3.008},
  author = {R{\"u}cknagel, Jessika and Vierkant, Paul and Ulrich, Robert and Kloska, Gabriele and Schnepf, Edeltraud and Fichtm{\"u}ller, David and Reuter, Evelyn and Semrau, Angelika and Kindling, Maxi and Pampel, H. and Witt, Michael and Fritze, Florian and Van De Sandt, Stephanie and Klump, Jens and Goebelbecker, Hans-J{\"u}rgen and Skarupianski, Michael and Bertelmann, Roland and Schirmbacher, Peter and Scholze, Frank and Kramer, Claudia and Fuchs, Claudio and Spier, Shaked and Kirchhoff, Agnes},
  year = {2015}
}

@misc{CrossRef,
  timestamp = {2019-12-22T22:00:38Z},
  title = {Relationships between {{DOIs}} and Other Objects},
  abstract = {Online publications and physical print publications with a digital fingerprint often do not exist as a single content item. Many parts comprise electronic publications which are often produced, cur...},
  language = {en-US},
  urldate = {2018-10-22},
  url = {http://support.crossref.org/hc/en-us/articles/214357426-Relationships-between-DOIs-and-other-objects},
  author = {{CrossRef}},
  file = {Snapshot:/home/vilhuber/Zotero/storage/FIG98SMS/214357426-Relationships-between-DOIs-and-other-objects.html:text/html}
}

@misc{JacobyInsideHigherEd2017,
  timestamp = {2019-12-22T22:00:39Z},
  title = {Should {{Journals Be Responsible}} for {{Reproducibility}}?},
  shorttitle = {Should {{Journals Be Responsible}} for {{Reproducibility}}?},
  abstract = {One of the top journals in political science makes data-sharing and replication part of the publication process.},
  language = {en},
  urldate = {2018-07-22},
  journal = {Inside Higher Ed},
  url = {https://www.insidehighered.com/blogs/rethinking-research/should-journals-be-responsible-reproducibility},
  author = {Jacoby, William G. and {Lafferty-Hess}, Sophia and Christian, Thu-Mai},
  month = jul,
  year = {2017},
  file = {Snapshot:/home/vilhuber/Zotero/storage/8ZDX4TAS/should-journals-be-responsible-reproducibility.html:text/html}
}

@article{McCullough2006-cz,
  timestamp = {2019-11-19T13:38:07Z},
  title = {Lessons from the {{JMCB Archive}}},
  volume = {38},
  issn = {0022-2879},
  abstract = {We examine the online archive of the Journal of Money, Credit, and Banking, in which an author is required to deposit the data and code that replicate the results of his paper. We find that most authors do not fulfill this requirement. Of more than 150 empirical articles, fewer than 15 could be replicated. Despite all this, there is no doubt that a data/code archive is more conducive to replicable research than the alternatives. We make recommendations to improve the functioning of the archive.},
  number = {4},
  journal = {Journal of Money, Credit, and Banking},
  doi = {10.1353/mcb.2006.0061},
  author = {McCullough, B D and McGeary, Kerry Anne and Harrison, Teresa D},
  month = jun,
  year = {2006},
  pages = {1093-1107}
}

@article{DataCiteMetadataWorkingGroup2017,
  timestamp = {2019-12-22T22:00:38Z},
  title = {{{DataCite Metadata Schema}} for the {{Publication}} and {{Citation}} of {{Research Data}} v4.1},
  language = {eng},
  doi = {10.5438/0015},
  author = {{DataCite Metadata Working Group}},
  editor = {Ashton, Jan and Barton, Amy and Birt, Noris and Dietiker, Stefanie and Elliot, Jannean and Fenner, Martin and Hugo, Wim and Jakobsson, Stefan and Bernal Mart{\'i}nez, Isabel and R{\"u}cknagel, Jessica and Yahia, Mohamed and Ziedorn, Frauke and Zolly, Lisa},
  year = {2017}
}

@article{DataCiteMetadataWorkingGroup2017a,
  timestamp = {2019-12-22T22:00:38Z},
  title = {{{DataCite Metadata Schema Documentation}} for the {{Publication}} and {{Citation}} of {{Research Data}} v4.1},
  abstract = {1 Introduction1.1 The DataCite Consortium1.2 DataCite Community Participation1.3 The Metadata Schema1.4 Version 4.1 Update2 DataCite Metadata Properties2.1 Overview2.2 Citation2.3 DataCite Properties3 XML Example4 XML Schema5 Other DataCite ServicesAppendicesAppendix 1: Controlled List DefinitionsAppendix 2: Earlier Version Update NotesAppendix 3: Standard values for unknown informationAppendix 4: Version 4.1 Changes in support of software citationAppendix 5: FORCE11 Software Citation Principles Mapping},
  language = {eng},
  doi = {10.5438/0014},
  author = {{DataCite Metadata Working Group}},
  editor = {Ashton, Jan and Barton, Amy and Birt, Noris and Dietiker, Stefanie and Elliot, Jannean and Fenner, Martin and Hugo, Wim and Jakobsson, Stefan and Bernal Mart{\'i}nez, Isabel and R{\"u}cknagel, Jessica and Yahia, Mohamed and Ziedorn, Frauke and Zolly, Lisa},
  year = {2017}
}

@misc{U.S.CensusBureau1999,
  timestamp = {2019-12-22T22:00:40Z},
  title = {Records {{Control Schedule}}: {{American Community Survey Records}}},
  publisher = {{National Archives and Records Administration}},
  url = {https://www.archives.gov/files/records-mgmt/rcs/schedules/departments/department-of-commerce/rg-0029/n1-029-98-001_sf115.pdf},
  author = {{U.S. Census Bureau}},
  month = apr,
  year = {1999}
}

@misc{U.S.CensusBureau2009,
  timestamp = {2019-12-22T22:00:40Z},
  title = {Records {{Control Schedule}}: {{Business Register}}},
  publisher = {{National Archives and Records Administration}},
  url = {https://www.archives.gov/records-mgmt/rcs/schedules/departments/department-of-commerce/rg-0029/n1-029-10-002_sf115.pdf},
  author = {{U.S. Census Bureau}},
  month = oct,
  year = {2009}
}

@misc{CrossRefSupportCenter,
  timestamp = {2019-12-22T22:00:38Z},
  title = {Metadata Deposit Schema 4.4.1},
  abstract = {Schema: crossref4.4.1.xsd
Full documentation: 4.4.1
Crossref included schema:~~

common4.4.1.xsd
fundref.xsd
AccessIndicators.xsd
clinicaltrials.xsd
relations.xsd
common4.3.5.xsd

External imported...},
  language = {en-US},
  urldate = {2018-06-14},
  journal = {Support Center},
  url = {http://support.crossref.org/hc/en-us/articles/115005251823-Metadata-deposit-schema-4-4-1},
  author = {{CrossRef}},
  file = {Snapshot:/home/vilhuber/Zotero/storage/J3KHZSC6/115005251823.html:text/html}
}

@misc{NationalCenterforBiotechnologyInformationU.S.NationalLibraryofMedicine2016,
  timestamp = {2019-12-22T22:00:39Z},
  title = {{{JATS}}: {{Journal Publishing Tag Set}}: V1.1},
  urldate = {2018-06-14},
  url = {https://jats.nlm.nih.gov/publishing/1.1/},
  author = {{National Center for Biotechnology Information, U.S. National Library of Medicine}},
  year = {2016}
}

@misc{CoreTrustSealCoreTrustSeal2017,
  timestamp = {2019-12-22T22:00:37Z},
  title = {Data {{Repositories Requirements}}},
  abstract = {CoreTrustSeal~Data Repositories Requirements The CoreTrustSeal~Data Repositories Requirements reflect the characteristics of trustworthy repositories. As such, all Requirements are mandatory and ar\ldots{}},
  language = {en-GB},
  urldate = {2018-06-14},
  journal = {CoreTrustSeal},
  url = {https://www.coretrustseal.org/why-certification/requirements/},
  author = {{CoreTrustSeal}},
  month = jan,
  year = {2017},
  file = {Snapshot:/home/vilhuber/Zotero/storage/KZBQZGZX/requirements.html:text/html}
}

@misc{Re3data.Org2015,
  timestamp = {2019-12-22T22:00:40Z},
  title = {Re3data.Org {{Metadata Schema}} 3.0 {{XML Schema}}},
  language = {eng},
  publisher = {{re3data.org}},
  doi = {10.2312/re3.009},
  author = {{Re3data.Org}},
  year = {2015}
}

@techreport{DataCitationSynthesisGroup2014,
  timestamp = {2019-12-22T22:00:38Z},
  title = {Joint {{Declaration}} of {{Data Citation Principles}}},
  abstract = {Sound, reproducible scholarship rests upon a foundation of
    robust, accessible data. For this to be so in practice as well as theory, data must be accorded
    due importance in the practice of scholarship and in the enduring scholarly record. In other words,
    data should be considered legitimate, citable products of research. Data citation, like the citation
    of other evidence and sources, is good research practice and is part of the scholarly ecosystem
    supporting data reuse.In support of this assertion, and to encourage good practice, we offer a set of guiding principles
    for data within scholarly literature, another dataset, or any other research object.},
  institution = {{Force11}},
  doi = {10.25490/a97f-egyk},
  author = {{Data Citation Synthesis Group} and Martone, Maryann},
  year = {2014}
}

@article{KidwellPLOSBiol.2016,
  timestamp = {2019-12-22T22:00:39Z},
  title = {Badges to {{Acknowledge Open Practices}}: {{A Simple}}, {{Low}}-{{Cost}}, {{Effective Method}} for {{Increasing Transparency}}},
  volume = {14},
  issn = {1545-7885},
  shorttitle = {Badges to {{Acknowledge Open Practices}}},
  language = {en},
  number = {5},
  journal = {PLOS Biology},
  doi = {10.1371/journal.pbio.1002456},
  author = {Kidwell, Mallory C. and Lazarevi{\'c}, Ljiljana B. and Baranski, Erica and Hardwicke, Tom E. and Piechowski, Sarah and Falkenberg, Lina-Sophia and Kennett, Curtis and Slowik, Agnieszka and Sonnleitner, Carina and {Hess-Holden}, Chelsey and Errington, Timothy M. and Fiedler, Susann and Nosek, Brian A.},
  editor = {Macleod, Malcolm R},
  month = may,
  year = {2016},
  pages = {e1002456}
}

@misc{BlohowiakOpenScienceFramework2013,
  timestamp = {2019-12-22T22:00:37Z},
  title = {Badges to {{Acknowledge Open Practices}}},
  abstract = {The aim is to specify a standard by which we can say that a scientific study has been conducted in accordance with open-science principles and provide visual icons to allow advertising of such good behaviours. 
    Hosted on the Open Science Framework},
  language = {en},
  urldate = {2018-06-04},
  journal = {Open Science Framework},
  url = {https://osf.io/tvyxz/},
  author = {Blohowiak, Ben B. and Cohoon, Johanna and {de-Wit}, Lee and Eich, Eric and Farach, Frank J. and Hasselman, Fred and Holcombe, Alex O. and Humphreys, Macartan and Lewis, Melissa and Nosek, Brian A.},
  month = feb,
  year = {2013},
  file = {Snapshot:/home/vilhuber/Zotero/storage/8MYKNRRF/tvyxz.html:text/html}
}

@article{CamererScience2016,
  timestamp = {2019-12-22T22:00:03Z},
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  copyright = {Copyright \textcopyright{} 2016, American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  abstract = {The reproducibility of scientific findings has been called into question. To contribute data about reproducibility in economics, we replicate 18 studies published in the American Economic Review and the Quarterly Journal of Economics in 2011-2014. All replications follow predefined analysis plans publicly posted prior to the replications, and have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We find a significant effect in the same direction as the original study for 11 replications (61\%); on average the replicated effect size is 66\% of the original. The reproducibility rate varies between 67\% and 78\% for four additional reproducibility indicators, including a prediction market measure of peer beliefs.},
  language = {en},
  journal = {Science},
  doi = {10.1126/science.aaf0918},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  month = mar,
  year = {2016},
  pages = {aaf0918},
  file = {Snapshot:/home/vilhuber/Zotero/storage/VYD777TH/science.html:text/html},
  pmid = {26940865}
}

@article{FanelliPNAS2018,
  timestamp = {2019-12-22T22:00:38Z},
  title = {Opinion: {{Is}} Science Really Facing a Reproducibility Crisis, and Do We Need It To?},
  volume = {115},
  copyright = {\textcopyright{} 2018 . Published under the PNAS license.},
  issn = {0027-8424, 1091-6490},
  shorttitle = {Opinion},
  abstract = {Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.},
  language = {en},
  number = {11},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1708272114},
  author = {Fanelli, Daniele},
  month = mar,
  year = {2018},
  keywords = {bias,crisis,integrity,misconduct,reproducible research},
  pages = {2628-2631},
  file = {Full Text PDF:/home/vilhuber/Zotero/storage/HZG53ZDA/Fanelli - 2018 - Opinion Is science really facing a reproducibilit.pdf:application/pdf;Snapshot:/home/vilhuber/Zotero/storage/BGPASNPP/2628.html:text/html},
  pmid = {29531051}
}

@article{Crosas2018,
  timestamp = {2019-12-22T22:00:38Z},
  title = {Data Policies of Highly-Ranked Social Science Journals},
  abstract = {By encouraging and requiring that authors share their data in order to publish articles, scholarly journals have become an important actor in the movement to improve the openness of data and the reproducibility of research. But how many social science journals encourage or mandate that authors share the data supporting their research findings? How does the share of journal data policies vary by discipline? What influences these journals' decisions to adopt such policies and instructions? And what do those policies and instructions look like?We discuss the results of our analysis of the instructions and policies of 291 highly-ranked journals publishing social science research, where we studied the contents of journal data policies and instructions across 14 variables, such as when and how authors are asked to share their data, and what role journal ranking and age play in the existence and quality of data policies and instructions. We also compare our results to the results of other studies that have analyzed the policies of social science journals, although differences in the journals chosen and how each study defines what constitutes a data policy limit this comparison.We conclude that a little more than half of the journals in our study have data policies. A greater share of the economics journals have data policies and mandate sharing, followed by political science/international relations and psychology journals.Finally, we use our findings to make several recommendations: Policies should include the terms ``data,'' ``dataset'' or more specific terms that make it clear what to make available; policies should include the benefits of data sharing; journals, publishers, and associations need to collaborate more to clarify data policies; and policies should explicitly ask for qualitative data.},
  doi = {10.17605/osf.io/9h7ay},
  author = {Crosas, Merc{\`e} and Gautier, Julian and Karcher, Sebastian and Kirilova, Dessi and Otalora, Gerard and Schwartz, Abigail},
  year = {2018}
}

@article{ChangAm.Econ.Rev.2017,
  timestamp = {2019-12-22T22:00:37Z},
  title = {A {{Preanalysis Plan}} to {{Replicate Sixty Economics Research Papers That Worked Half}} of the {{Time}}},
  volume = {107},
  issn = {0002-8282},
  abstract = {We attempted to replicate 67 macroeconomic papers using author-provided data and code files by following a preanalysis plan. Excluding 6 papers that used confidential data, we obtained data and code replication files for 29 of 35 papers (83 percent) that were required to provide such files as a condition of publication, compared to 11 of 26 papers (42 percent) that were not required to provide such files. Also excluding the 2 papers that used software we did not possess, we replicated 29 of 59 papers (49 percent) with assistance from the authors. We conclude with recommendations on improving replication of economics research.},
  language = {en},
  number = {5},
  journal = {American Economic Review},
  doi = {10.1257/aer.p20171034},
  author = {Chang, Andrew C. and Li, Phillip},
  month = may,
  year = {2017},
  pages = {60-64},
  file = {Snapshot:/home/vilhuber/Zotero/storage/49FQLHTH/articles.html:text/html}
}

@article{HofflerAm.Econ.Rev.2017,
  timestamp = {2019-12-22T22:00:38Z},
  title = {Replication and {{Economics Journal Policies}}},
  volume = {107},
  issn = {0002-8282},
  abstract = {Economics journals with reproducibility policies are cited more often than others. For the minority of journals with a mandatory and enforced policy, this is significant when controlling for time and journal effects. To cope with the large variety of software used and to develop standards for replicability, joint efforts of journals could ensure each empirical study is published with data, code, and instructions on how to use them together. Individual reviewers could take initiative by asking for replicable empirical results. The American Journal of Political Science sets an example by having all empirical studies externally check for replicability prior to publication.},
  language = {en},
  number = {5},
  journal = {American Economic Review},
  doi = {10.1257/aer.p20171032},
  author = {H{\"o}ffler, Jan H.},
  month = may,
  year = {2017},
  pages = {52-55},
  file = {Snapshot:/home/vilhuber/Zotero/storage/WQTTEPNR/articles.html:text/html}
}

@article{HamermeshAm.Econ.Rev.2017,
  timestamp = {2019-12-22T22:00:38Z},
  title = {Replication in {{Labor Economics}}: {{Evidence}} from {{Data}}, and {{What It Suggests}}},
  volume = {107},
  issn = {0002-8282},
  shorttitle = {Replication in {{Labor Economics}}},
  abstract = {Examining the most heavily cited publications in labor economics from the early 1990s, I show that few of over 3,000 articles, citing them directly, replicates them. They are replicated more frequently using data from other time periods and economies, so that the validity of their central ideas has typically been verified. This pattern of scholarship suggests, beyond the currently required depositing of data and code upon publication, that there is little need for formal mechanisms for replication. The market for scholarship already produces replications of non-laboratory applied research.},
  language = {en},
  number = {5},
  journal = {American Economic Review},
  doi = {10.1257/aer.p20171121},
  author = {Hamermesh, Daniel S.},
  month = may,
  year = {2017},
  pages = {37-40},
  file = {Snapshot:/home/vilhuber/Zotero/storage/RVD3DWI2/articles.html:text/html}
}

@article{CramerAm.Econ.Rev.2016,
  timestamp = {2019-12-22T22:00:37Z},
  title = {Disruptive {{Change}} in the {{Taxi Business}}: {{The Case}} of {{Uber}}},
  volume = {106},
  issn = {0002-8282},
  shorttitle = {Disruptive {{Change}} in the {{Taxi Business}}},
  abstract = {In most cities, the taxi industry is highly regulated and has restricted entry. Ride sharing services, such as Uber and Lyft, which use mobile internet technology to connect passengers and drivers, have begun to compete with traditional taxis. This paper examines the efficiency of ride sharing services vis-a-vis taxis. In most cities with data available, UberX drivers spend a significantly higher fraction of their time, and drive a substantially higher share of miles, with a passenger in their car than do taxi drivers. Reasons for this efficiency advantage are explored.},
  language = {en},
  number = {5},
  journal = {American Economic Review},
  doi = {10.1257/aer.p20161002},
  author = {Cramer, Judd and Krueger, Alan B.},
  month = may,
  year = {2016},
  pages = {177-182},
  file = {Snapshot:/home/vilhuber/Zotero/storage/YULZ5FM5/articles.html:text/html}
}

@techreport{Bollen2015,
  timestamp = {2019-12-22T22:00:37Z},
  type = {Report of the {{Subcommittee}} on {{Replicability}} in {{Science Advisory Committee}} to the {{National Science Foundation Directorate}} for {{Social}}, {{Behavioral}}, and {{Economic Sciences}}},
  title = {Social, {{Behavioral}}, and {{Economic Sciences Perspectives}} on {{Robust}} and {{Reliable Science}}},
  urldate = {2018-05-20},
  institution = {{National Science Foundation}},
  url = {https://www.nsf.gov/sbe/AC_Materials/SBE_Robust_and_Reliable_Research_Report.pdf},
  author = {Bollen, Kenneth and Cacioppo, John T. and Kaplan, Robert M. and Krosnick, Jon A. and Olds, James L.},
  year = {2015},
  file = {SBE_Robust_and_Reliable_Research_Report.pdf:/home/vilhuber/Zotero/storage/GN4XX77B/SBE_Robust_and_Reliable_Research_Report.pdf:application/pdf}
}

@article{Collaboration2015-ev,
  timestamp = {2019-11-25T02:11:12Z},
  title = {Estimating the Reproducibility of Psychological Science},
  volume = {349},
  issn = {0036-8075},
  number = {6251},
  journal = {Science},
  doi = {10.1126/science.aac4716},
  author = {Collaboration, Open Science},
  month = aug,
  year = {2015},
  pages = {aac4716-aac4716},
  publisher = {American Association for the Advancement of Science (AAAS)}
}

@article{Baker2015-sh,
  timestamp = {2019-11-25T02:11:12Z},
  title = {Over Half of Psychology Studies Fail Reproducibility Test},
  issn = {0028-0836},
  journal = {Nature},
  doi = {10.1038/nature.2015.18248},
  author = {Baker, Monya},
  month = aug,
  year = {2015},
  publisher = {Nature Publishing Group}
}

@article{Nature_Editorial_Board2014-iv,
  timestamp = {2019-11-25T02:11:12Z},
  title = {Journals Unite for Reproducibility},
  volume = {515},
  issn = {0028-0836},
  number = {7525},
  journal = {Nature},
  doi = {10.1038/515007a},
  author = {{Nature Editorial Board}},
  month = nov,
  year = {2014},
  pages = {7-7},
  publisher = {Nature Publishing Group}
}

@article{StoddenPNAS2018,
  timestamp = {2019-12-22T22:00:40Z},
  title = {An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility},
  copyright = {\textcopyright{} 2018 . Published under the PNAS license.},
  issn = {0027-8424, 1091-6490},
  abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (i) requesting data and code from authors and (ii) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal Science after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy\textemdash{}author remission of data and code postpublication upon request\textemdash{}an improvement over no policy, but currently insufficient for reproducibility.},
  language = {en},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1708290115},
  author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
  month = mar,
  year = {2018},
  keywords = {open science,data access,reproducible research,code access,reproducibility policy},
  pages = {201708290},
  file = {Full Text PDF:/home/vilhuber/Zotero/storage/TL3GDR5C/Stodden et al. - 2018 - An empirical analysis of journal policy effectiven.pdf:application/pdf;Snapshot:/home/vilhuber/Zotero/storage/HDZRRTYJ/1708290115.html:text/html},
  pmid = {29531050}
}

@misc{HagstromFORCE112014,
  timestamp = {2019-12-22T22:00:38Z},
  title = {The {{FAIR Data Principles}}},
  abstract = {Join in the discussion - leave your comments below FAIR Data Principles},
  language = {en},
  urldate = {2018-05-20},
  journal = {FORCE11},
  url = {https://www.force11.org/group/fairgroup/fairprinciples},
  author = {Hagstrom, Stephanie},
  month = sep,
  year = {2014},
  file = {Snapshot:/home/vilhuber/Zotero/storage/2FBC2LYC/fairprinciples.html:text/html}
}

@article{CraginPhilosophicalTransactionsoftheRoyalSocietyA:MathematicalPhysicalandEngineeringSciences2010,
  timestamp = {2019-12-22T22:00:37Z},
  title = {Data Sharing, Small Science and Institutional Repositories},
  volume = {368},
  abstract = {Results are presented from the Data Curation Profiles project research, on who is willing to share what data with whom and when. Emerging from scientists' discussions on sharing are several dimensions suggestive of the variation in both what it means `to share' and how these processes are carried out. This research indicates that data curation services will need to accommodate a wide range of subdisciplinary data characteristics and sharing practices. As part of a larger set of strategies emerging across academic institutions, institutional repositories (IRs) will contribute to the stewardship and mobilization of scientific research data for e-Research and learning. There will be particular types of data that can be managed well in an IR context when characteristics and practices are well understood. Findings from this study elucidate scientists' views on `sharable' forms of data\textemdash{}the particular representation that they view as most valued for reuse by others within their own research areas\textemdash{}and the anticipated duration for such reuse. Reported sharing incidents that provide insights into barriers to sharing and related concerns on data misuse are included.},
  number = {1926},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  doi = {10.1098/rsta.2010.0165},
  author = {Cragin, Melissa H. and Palmer, Carole L. and Carlson, Jacob R. and Witt, Michael},
  month = sep,
  year = {2010},
  pages = {4023-4038},
  file = {Full Text PDF:/home/vilhuber/Zotero/storage/NGNDIFGY/Cragin et al. - 2010 - Data sharing, small science and institutional repo.pdf:application/pdf;Snapshot:/home/vilhuber/Zotero/storage/862Z3VRG/rsta.2010.html:text/html}
}

@article{CousijnSci.Data2018,
  timestamp = {2019-12-22T22:00:37Z},
  title = {A Data Citation Roadmap for Scientific Publishers},
  volume = {5},
  copyright = {2018 Nature Publishing Group},
  issn = {2052-4463},
  abstract = {This article presents a practical roadmap for scholarly publishers to implement data citation in accordance with the Joint Declaration of Data Citation Principles (JDDCP), a synopsis and harmonization of the recommendations of major science policy bodies. It was developed by the Publishers Early Adopters Expert Group as part of the Data Citation Implementation Pilot (DCIP) project, an initiative of FORCE11.org and the NIH BioCADDIE program. The structure of the roadmap presented here follows the ``life of a paper'' workflow and includes the categories Pre-submission, Submission, Production, and Publication. The roadmap is intended to be publisher-agnostic so that all publishers can use this as a starting point when implementing JDDCP-compliant data citation. Authors reading this roadmap will also better know what to expect from publishers and how to enable their own data citations to gain maximum impact, as well as complying with what will become increasingly common funder mandates on data transparency.},
  language = {en},
  journal = {Scientific Data},
  doi = {10.1038/sdata.2018.259},
  author = {Cousijn, Helena and Kenall, Amye and Ganley, Emma and Harrison, Melissa and Kernohan, David and Lemberger, Thomas and Murphy, Fiona and Polischuk, Patrick and Taylor, Simone and Martone, Maryann and Clark, Tim},
  month = nov,
  year = {2018},
  pages = {180259},
  file = {Full Text PDF:/home/vilhuber/Zotero/storage/BV85XZGS/Cousijn et al. - 2018 - A data citation roadmap for scientific publishers.pdf:application/pdf;Full Text PDF:/home/vilhuber/Zotero/storage/E88AZUFN/Cousijn et al. - 2018 - A data citation roadmap for scientific publishers.pdf:application/pdf;Full Text PDF:/home/vilhuber/Zotero/storage/NK2SS46W/Cousijn et al. - 2018 - A data citation roadmap for scientific publishers.pdf:application/pdf;Full Text PDF:/home/vilhuber/Zotero/storage/Q227LALW/Cousijn et al. - 2018 - A data citation roadmap for scientific publishers.pdf:application/pdf;Snapshot:/home/vilhuber/Zotero/storage/7KXM76J5/sdata2018259.html:text/html;Snapshot:/home/vilhuber/Zotero/storage/I9LC6B68/sdata2018259.html:text/html;Snapshot:/home/vilhuber/Zotero/storage/NH9UR6W2/Cousijn et al. - 2018 - A data citation roadmap for scientific publishers.html:text/html;Snapshot:/home/vilhuber/Zotero/storage/RBEUIP87/sdata2018259.html:text/html}
}

@techreport{Christensen2019,
  timestamp = {2019-12-22T22:00:37Z},
  type = {Preprint},
  title = {Open {{Science Practices}} Are on the {{Rise}}: {{The State}} of {{Social Science}} ({{3S}}) {{Survey}}},
  shorttitle = {Open {{Science Practices}} Are on the {{Rise}}},
  abstract = {Has there been meaningful movement toward open science practices within the social sciences in recent years? Discussions about changes in practices such as posting data and pre-registering analyses have been marked by controversy\textemdash{}including controversy over the extent to which change has taken place. This study, based on the State of Social Science (3S) Survey, provides the first comprehensive assessment of awareness of, attitudes towards, perceived norms regarding, and adoption of open science practices within a broadly representative sample of scholars from four major social science disciplines: economics, political science, psychology, and sociology. We observe a steep increase in adoption: as of 2017, over 80\% of scholars had used at least one such practice, rising from one quarter a decade earlier. Attitudes toward research transparency are on average similar between older and younger scholars, but the paceof change differs by field and methodology. According with theories of normal science and scientific change, the timing of increases in adoption coincides with technological innovations and institutional policies. Patterns are consistent with most scholars underestimating the trend toward open science in their discipline.},
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/5rksu},
  author = {Christensen, Garret and Wang, Zenan and Paluck, Elizabeth Levy and Swanson, Nicholas and Birke, David J. and Miguel, Edward and Littman, Rebecca},
  month = oct,
  year = {2019}
}

@techreport{CenterforOpenScience2016,
  timestamp = {2019-12-22T22:00:37Z},
  title = {{{TOP Guidelines}} Summary Table},
  abstract = {This component contains resources relevant to journals implementing the TOP Guidelines 
    Hosted on the Open Science Framework},
  language = {en},
  urldate = {2019-11-19},
  institution = {{Center for Open Science}},
  url = {https://osf.io/kgnva/},
  author = {{Center for Open Science}},
  month = jun,
  year = {2016},
  file = {Snapshot:/home/vilhuber/Zotero/storage/4XLXZQWQ/edfr9.html:text/html}
}

@misc{AmericanEconomicAssociation2019,
  timestamp = {2019-12-22T22:00:37Z},
  title = {Data and {{Code Availability Policy}}},
  urldate = {2019-11-19},
  url = {https://www.aeaweb.org/journals/policies/data-code},
  author = {{American Economic Association}},
  month = jul,
  year = {2019},
  file = {American Economic Association:/home/vilhuber/Zotero/storage/4WMJD2BJ/data-code.html:text/html}
}

@misc{Mellor2018a,
  timestamp = {2019-12-22T22:00:39Z},
  title = {The {{Landscape}} of {{Open Data Policies}}},
  urldate = {2019-11-19},
  url = {https://cos.io/blog/landscape-open-data-policies/},
  author = {Mellor, David},
  month = aug,
  year = {2018},
  file = {The Landscape of Open Data Policies:/home/vilhuber/Zotero/storage/KWEVGJNY/landscape-open-data-policies.html:text/html}
}

@misc{FAIRsharing2017,
  timestamp = {2019-12-22T22:00:38Z},
  title = {{{FAIRsharing Recommendation}}: {{PLOS}}},
  urldate = {2019-12-02},
  url = {https://fairsharing.org/recommendation/PLOS},
  author = {{FAIRsharing}},
  year = {2017},
  file = {FAIRsharing Recommendation\: PLOS:/home/vilhuber/Zotero/storage/94I4TMQH/PLOS.html:text/html}
}

@misc{PublicLibraryofScience,
  timestamp = {2019-12-22T22:00:39Z},
  title = {{{PLOS ONE Recommended Repositories}}},
  urldate = {2019-12-02},
  url = {https://journals.plos.org/plosone/s/data-availability\#loc-recommended-repositories},
  author = {{Public Library of Science}},
  file = {PLOS ONE\: accelerating the publication of peer-reviewed science:/home/vilhuber/Zotero/storage/9KX2WJYQ/data-availability.html:text/html}
}

@techreport{Kingi2018,
  timestamp = {2019-12-22T22:00:39Z},
  address = {{Berkeley, CA}},
  type = {Presentation},
  title = {The {{Reproducibility}} of {{Economics Research}}:  {{A Case Study}}},
  url = {https://osf.io/srg57/},
  author = {Kingi, Hautahi and Stanchi, Flavio and Vilhuber, Lars and Herbert, Sylverie},
  year = {2018}
}

@misc{nicholaseubankThePoliticalMethodologist2014,
  timestamp = {2019-12-22T22:00:39Z},
  title = {A {{Decade}} of {{Replications}}: {{Lessons}} from the {{Quarterly Journal}} of {{Political Science}}},
  shorttitle = {A {{Decade}} of {{Replications}}},
  abstract = {Editor's note: this piece is contributed by Nicholas Eubank, a PhD Candidate in Political Economy at the Stanford University Graduate School of Business. The success of science depends critic\ldots{}},
  language = {en},
  urldate = {2019-12-20},
  journal = {The Political Methodologist},
  url = {https://thepoliticalmethodologist.com/2014/12/09/a-decade-of-replications-lessons-from-the-quarterly-journal-of-political-science/},
  author = {{nicholaseubank}},
  month = dec,
  year = {2014},
  file = {Snapshot:/home/vilhuber/Zotero/storage/32QALIA3/a-decade-of-replications-lessons-from-the-quarterly-journal-of-political-science.html:text/html}
}

@article{MielePLOSComputationalBiology2019,
  timestamp = {2019-12-22T22:00:39Z},
  title = {Nine Quick Tips for Analyzing Network Data},
  volume = {15},
  issn = {1553-7358},
  language = {en},
  number = {12},
  journal = {PLOS Computational Biology},
  doi = {10.1371/journal.pcbi.1007434},
  author = {Miele, Vincent and Matias, Catherine and Robin, St{\'e}phane and Dray, St{\'e}phane},
  month = dec,
  year = {2019},
  keywords = {Software tools,Biologists,Food web structure,Genetic networks,Mathematical models,Network analysis,Neural networks,Protein interaction networks},
  pages = {e1007434},
  file = {Miele et al_2019_Nine quick tips for analyzing network data.pdf:/home/vilhuber/Zotero/storage/ZU362QAF/Miele et al_2019_Nine quick tips for analyzing network data.pdf:application/pdf;Snapshot:/home/vilhuber/Zotero/storage/5E9WA99N/article.html:text/html}
}

@article{American_Economic_Association2008-wayback,
  timestamp = {2019-12-22T21:40:29Z},
  title = {Data Availability Policy},
  urldate = {2019-09-21},
  url = {https://web.archive.org/web/20180927113622/https://www.aeaweb.org/journals/policies/data-availability-policy},
  author = {{American Economic Association}},
  year = {2008},
  nonote = {(accessed: 2018-09-27 via Archive.org)}
}

@article{10.1257/pandp.108.745,
  timestamp = {2019-12-22T21:40:30Z},
  title = {Report of the Search Committee to Appoint a Data Editor for the {{AEA}}},
  volume = {108},
  journal = {AEA Papers and Proceedings},
  doi = {10.1257/pandp.108.745},
  author = {Duflo, Esther and Hoynes, Hilary},
  year = {2018},
  pages = {745}
}

@misc{force11declaration,
  timestamp = {2019-12-22T21:40:31Z},
  title = {Data Citation Synthesis Group: Joint Declaration of Data Citation Principles},
  journal = {FORCE11},
  doi = {10.25490/a97f-egyk},
  author = {Martone, M. (ed.)},
  year = {2014}
}

@book{Christensen2019a,
  timestamp = {2019-12-22T22:13:14Z},
  address = {{Oakland, California}},
  title = {Transparent and Reproducible Social Science Research: How to Do Open Science},
  isbn = {978-0-520-96923-0},
  lccn = {Q180.55.S7},
  shorttitle = {Transparent and Reproducible Social Science Research},
  abstract = {"Social science practitioners have recently witnessed numerous episodes of influential research that fell apart upon close scrutiny. These instances have spurred suspicions that other published results may contain errors or may at least be less robust than they appear. In response, an influential movement has emerged across the social sciences for greater research transparency, openness, and reproducibility. Transparent and Reproducible Social Science Research crystallizes the new insights, practices, and methods of this rising interdisciplinary field"--Provided by publisher},
  publisher = {{University of California Press}},
  author = {Christensen, Garret S. and Freese, Jeremy and Miguel, Edward},
  year = {2019},
  keywords = {Reproducible research,Research,Social sciences}
}

@misc{NatureScientificData2019,
  timestamp = {2019-12-23T03:14:55Z},
  title = {Scientific {{Data Repository Questionnaire}}},
  urldate = {2019-12-22},
  url = {https://www.nature.com/documents/scidata-repository-questionnaire.docx},
  author = {{Nature Scientific Data}},
  month = jul,
  year = {2019}
}

@article{VilhuberAEAPap.Proc.2020,
  timestamp = {2019-12-23T04:16:06Z},
  title = {Report by the {{AEA Data Editor}} (2020)},
  volume = {110},
  issn = {2574-0768, 2574-0776},
  language = {en},
  journal = {AEA Papers and Proceedings},
  author = {Vilhuber, Lars},
  year = {forthcoming}
}


